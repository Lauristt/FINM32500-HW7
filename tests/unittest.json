{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Project Unit Tests (.ipynb)\n",
    "\n",
    "This notebook contains the unit tests for the financial analytics project, fulfilling Task 6.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Place this notebook inside the `tests/` folder.\n",
    "2.  Ensure all the project modules (`metrics.py`, `parallel.py`, `portfolio.py`, etc.) are in the parent directory.\n",
    "3.  Run all cells (`Cell` > `Run All`) to execute the test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n",
    "import os\n",
    "import concurrent.futures\n",
    "from pandas.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Path Correction ---\n",
    "# This is necessary so the notebook in 'tests/' can find the .py files\n",
    "# in the parent directory.\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    print(f\"Adding parent directory to system path: {parent_dir}\")\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# --- Import Functions from Our Modules ---\n",
    "try:\n",
    "    from metrics import compute_rolling_pandas, compute_rolling_polars, compute_drawdown\n",
    "    from parallel import sequential_execution, parallel_execution\n",
    "    from portfolio import process_portfolio_sequentially\n",
    "except ImportError as e:\n",
    "    print(f\"Error: Could not import modules. {e}\")\n",
    "    print(\"Please make sure this notebook is in a 'tests' folder and the .py files are in the parent directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Case Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestProjectAnalytics(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Set up small, predictable mock data for all tests.\"\"\"\n",
    "        \n",
    "        # --- Data for Rolling Metrics & Parallel Tests ---\n",
    "        self.test_data_pd = pd.DataFrame({\n",
    "            'timestamp': pd.to_datetime([\n",
    "                '2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05',\n",
    "                '2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'\n",
    "            ]),\n",
    "            'symbol': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],\n",
    "            'price': [100, 102, 101, 103, 104, 50, 50, 51, 52, 51]\n",
    "        }).set_index('timestamp')\n",
    "        \n",
    "        self.test_data_pl = pl.DataFrame({\n",
    "            'timestamp': pd.to_datetime([\n",
    "                '2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05',\n",
    "                '2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'\n",
    "            ]),\n",
    "            'symbol': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],\n",
    "            'price': [100, 102, 101, 103, 104, 50, 50, 51, 52, 51]\n",
    "        }).sort('timestamp')\n",
    "        \n",
    "        # --- Data for Portfolio Aggregation Test ---\n",
    "        self.mock_latest_data = {\n",
    "            'TEST1': {'price': 100},\n",
    "            'TEST2': {'price': 200}\n",
    "        }\n",
    "        self.mock_price_history = {\n",
    "            'TEST1': pd.Series([90, 100, 80, 100]), # Max DD: (80-100)/100 = -0.2\n",
    "            'TEST2': pd.Series([180, 200, 210, 190]) # Max DD: (190-210)/210 = -0.0952\n",
    "        }\n",
    "\n",
    "    def test_pandas_rolling_sma(self):\n",
    "        \"\"\"1. Validate correctness of rolling metrics (Pandas).\"\"\"\n",
    "        df = compute_rolling_pandas(self.test_data_pd.copy(), window=3)\n",
    "        \n",
    "        # Check 'A' at 2023-01-03\n",
    "        val_a = df[df['symbol'] == 'A']['sma_20'].iloc[2]\n",
    "        expected_sma_a = (100 + 102 + 101) / 3\n",
    "        self.assertAlmostEqual(val_a, expected_sma_a)\n",
    "        \n",
    "        # Check 'B' at 2023-01-04\n",
    "        val_b = df[df['symbol'] == 'B']['sma_20'].iloc[3]\n",
    "        expected_sma_b = (50 + 51 + 52) / 3\n",
    "        self.assertAlmostEqual(val_b, expected_sma_b)\n",
    "\n",
    "    def test_polars_rolling_sma_equivalence(self):\n",
    "        \"\"\"2. Test pandas vs polars outputs for equivalence.\"\"\"\n",
    "        df_pl = compute_rolling_polars(self.test_data_pl.clone(), window=3)\n",
    "        \n",
    "        # Check 'A' at 2023-01-03\n",
    "        val_a_pl = df_pl.filter(pl.col('symbol') == 'A')['sma_20'][2]\n",
    "        expected_sma_a = (100 + 102 + 101) / 3\n",
    "        self.assertAlmostEqual(val_a_pl, expected_sma_a)\n",
    "        \n",
    "        # Check 'B' at 2023-01-04\n",
    "        val_b_pl = df_pl.filter(pl.col('symbol') == 'B')['sma_20'][3]\n",
    "        expected_sma_b = (50 + 51 + 52) / 3\n",
    "        self.assertAlmostEqual(val_b_pl, expected_sma_b)\n",
    "\n",
    "    def test_parallel_consistency(self):\n",
    "        \"\"\"3. Confirm threading and multiprocessing produce consistent results.\"\"\"\n",
    "        # Note: We use the *original* df here, as the parallel functions\n",
    "        # are designed to handle the full, unsorted, multi-symbol DataFrame.\n",
    "        df_seq = sequential_execution(self.test_data_pd.copy())\n",
    "        df_thread = parallel_execution(self.test_data_pd.copy(), concurrent.futures.ThreadPoolExecutor)\n",
    "        df_proc = parallel_execution(self.test_data_pd.copy(), concurrent.futures.ProcessPoolExecutor)\n",
    "        \n",
    "        # Use pandas' testing utility to compare DataFrames\n",
    "        assert_frame_equal(df_seq, df_thread)\n",
    "        assert_frame_equal(df_seq, df_proc)\n",
    "\n",
    "    def test_portfolio_aggregation(self):\n",
    "        \"\"\"4. Ensure portfolio aggregation matches expected totals.\"\"\"\n",
    "        test_portfolio = {\n",
    "          \"name\": \"Test Portfolio\",\n",
    "          \"positions\": [\n",
    "            {\"symbol\": \"TEST1\", \"quantity\": 10} # Value = 10 * 100 = 1000\n",
    "          ],\n",
    "          \"sub_portfolios\": [{\n",
    "              \"name\": \"Sub\",\n",
    "              \"positions\": [\n",
    "                {\"symbol\": \"TEST2\", \"quantity\": 5} # Value = 5 * 200 = 1000\n",
    "              ],\n",
    "              \"sub_portfolios\": []\n",
    "          }]\n",
    "        }\n",
    "        \n",
    "        result = process_portfolio_sequentially(\n",
    "            test_portfolio, \n",
    "            self.mock_latest_data, \n",
    "            self.mock_price_history\n",
    "        )\n",
    "        \n",
    "        # Test top-level aggregation\n",
    "        self.assertAlmostEqual(result['total_value'], 2000.0) # 1000 + 1000\n",
    "        \n",
    "        # Test max drawdown (should be worst of all components)\n",
    "        # TEST1 DD = -0.2, TEST2 DD = -0.0952\n",
    "        self.assertAlmostEqual(result['max_drawdown'], -0.2)\n",
    "        \n",
    "        # Test sub-portfolio aggregation\n",
    "        sub_portfolio_result = result['sub_portfolios'][0]\n",
    "        self.assertAlmostEqual(sub_portfolio_result['total_value'], 1000.0)\n",
    "        self.assertAlmostEqual(sub_portfolio_result['max_drawdown'], -0.0952, places=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the standard way to run unittest in a Jupyter notebook\n",
    "suite = unittest.TestSuite()\n",
    "suite.addTest(unittest.makeSuite(TestProjectAnalytics))\n",
    "\n",
    "# Create a TextTestRunner and capture output in a string buffer\n",
    "runner = unittest.TextTestRunner(stream=io.StringIO(), verbosity=2)\n",
    "result = runner.run(suite)\n",
    "\n",
    "# Print the captured output\n",
    "print(\"--- Unit Test Results ---\")\n",
    "print(result.stream.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}